The follow content be provide under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resource for free. To make a donation or view additional material from hundred of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: We're go to start a brand new, excite topic, dynamic programming. AUDIENCE: Yes! PROFESSOR: Yeah! So exciting. Actually, I be really excite because dynamic program be my favorite thing in the world, in algorithms. And it's go to be the next four lectures, it's so exciting. It have lot of different facets. It's a very general, powerful design technique. We don't talk a lot about algorithm design in this class, but dynamic program be one that's so important. And also take a little while to settle in. We like to inject it into you now, in 006. So in general, our motivation be design new algorithm and dynamic programming, also call DP, be a great way-- or a very general, powerful way to do this. It's especially good, and intend for, optimization problems, thing like shortest paths. You want to find the best way to do something. Shortest path be you want to find the shortest path, the minimum-length path. You want to minimize, maximize something, that's an optimization problem, and typically good algorithm to solve them involve dynamic programming. It's a bit of a broad statement. You can also think of dynamic program a a kind of exhaustive search. Which be usually a bad thing to do because it lead to exponential time. But if you do it in a clever way, via dynamic programming, you typically get polynomial time. So one perspective be that dynamic program be approximately careful brute force. It's kind of a funny combination. A bit of an oxymoron. But we take the idea of brute force, which is, try all possibility and you do it carefully and you get it to polynomial time. There be a lot of problem where essentially the only know polynomial time algorithm be via dynamic programming. It doesn't always work, there's some problem where we don't think there be polynomial time algorithms, but when it's possible DP be a nice, sort of, general approach to it. And we're go to be talk a lot about dynamic programming. There's a lot of different way to think about it. We'll look at a few today. We're go to warm up today with some fairly easy problem that we already know how to solve, namely compute Fibonacci numbers. It's pretty easy. And compute shortest paths. And then in the next three lecture we're go to get to more interest example where it's pretty surprising that you can even solve the problem in polynomial time. Probably the first burning question on your mind, though, be why be it call dynamic programming? What do that even mean? And I use to have this spiel about, well, you know, program refers to the-- I think it's the British notion of the word, where it's about optimization. Optimization in American English be something like program in British English, where you want to set up the program-- the schedule for your train or something, where program come from originally. But I look up the actual history of, why be it call dynamic programming. Dynamic program be invent by a guy name Richard Bellman. You may have heard of Bellman in the Bellman-Ford algorithm. So this be actually the precursor to Bellman-Ford. And we're go to see Bellman-Ford come up naturally in this setting. So here's a quote about him. It says, Bellman explain that he invent the name dynamic program to hide the fact that he be do mathematical research. He be work at this place call Rand, and under a secretary of defense who have a pathological fear and hatred for the term research. So he settle on the term dynamic program because it would be difficult to give a pejorative meaning to it. And because it be something not even a congressman could object to. Basically, it sound cool. So that's the origin of the name dynamic programming. So why be the call that? Who knows. I mean, now you know. But it's not-- it's a weird term. Just take it for what it is. It may make some kind of sense, but-- All right. So we be go to start with this example of how to compute Fibonacci numbers. And maybe before we actually start I'm go to give you a sneak peak of what you can think of dynamic program as. And this equation, so to speak, be go to change throughout today's lecture. In the end we'll settle on a sort of more accurate perspective. The basic idea of dynamic program be to take a problem, split it into subproblems, solve those subproblems, and reuse the solution to your subproblems. It's like a lesson in recycling. So we'll see that in Fibonacci numbers. So you remember Fibonacci numbers, right? The number of rabbit you have on day n, if they reproduce. We've mention them before, we're talk about AVL trees, I think. So this be the usual-- you can think of it a a recursive definition or recurrence on Fibonacci numbers. It's the definition of what the nth Fibonacci number is. So let's suppose our goal-- an algorithmic problem is, compute the nth Fibonacci number. And I'm go to assume here that that fit in a word. And so basic arithmetic, addition, whatever's constant time per operation. So how do we do it? You all know how to do it. Anyways-- but I'm go to give you the dynamic program perspective on things. So this will seem kind of obvious, but it is-- we're go to apply exactly the same principle that we will apply over and over in dynamic programming. But here it's in a very familiar setting. So we're go to start with the naive recursive algorithm. And that is, if you want to compute the nth Fibonacci number, you check whether you're in the base case. I'm go to write it this way. So f be just our return value. You'll see why I write it this way in a moment. Then you return f. In the base case it's 1, otherwise you recursively call Fibonacci of n minus 1. You recursively call Fibonacci of n minus 2. Add them together, return that. This be a correct algorithm. Is it a good algorithm? No. It's very bad. Exponential time. How do we know it's exponential time, other than from experience? Well, we can write the run time a recurrence. T of n represent the time to compute the nth Fibonacci number. How can I write the recurrence? You're gonna throwback to the early lectures, divide and conquer. I hear whispers. Yeah? AUDIENCE: [INAUDIBLE]. PROFESSOR: Yeah. T of n minus 1 plus t of n minus 2 plus constant. I don't know how many you have by now. So to create the nth Fibonacci number we have to compute the n minus first Fibonacci number, and the n minus second Fibonacci number. That's these two recursions. And then we take constant time otherwise. We do constant number of additions, comparisons. Return all these operations-- take constant time. So that's a recurrence. How do we solve this recurrence? Well one way be to see this be the Fibonacci recurrence. So it's the same thing. There's this plus whatever. But in particular, this be at least the nth Fibonacci number. And if you know Fibonacci stuff, that's about the golden ratio to the nth power. Which be bad. We have a similar recurrence in AVL trees. And so another way to solve it-- it's just good review-- say, oh well, that's at least 2 time t of n minus 2. Because it's go to be monotone. The big n is, the more work you have to do. Because to do the nth thing you have to do the n minus first thing. So we could just reduce t of n minus 1 to t of n minus 2. That will give u a low bound. And now these two terms-- now this be sort of an easy thing. You see that you're multiply by 2 each time. You're subtract How many time can I subtract 2 from n? N/2 times, before I get down to a constant. And so this be equal to 2 to the n over 2-- I mean, time some constant, which be what you get in the base case. So I guess I should say theta. This thing be theta that. OK. So it's at least that big. And the right constant be phi. And the base of the exponent. OK. So that's a bad algorithm. We all know it's a bad algorithm. But I'm go to give you a general approach for make bad algorithm like this good. And that general approach be call memoization. We'll go over here. And this be a technique of dynamic programming. So I'm go to call this the memoized dynamic program algorithm. So do I settle on use memo in the notes? Yeah. The idea be simple. Whenever we compute a Fibonacci number we put it in a dictionary. And then when we need to compute the nth Fibonacci number we check, be it already in the dictionary? Did we already solve this problem? If so, return that answer. Otherwise, computer it. You'll see the transformation be very simple. OK. These two line be identical to these two lines. So you can see how the transformation work in general. You could do this with any recursive algorithm. The memoization transformation on that algorithm-- which is, we initially make an empty dictionary call memo. And before we actually do the computation we say, well, check whether this version of the Fibonacci problem, compute f of n, be already in our dictionary. So if that key be already in the dictionary, we return the correspond value in the dictionary. And then once we've compute the nth Fibonacci number, if we bother to do this, if this didn't apply, then we store it in the memo table. So we say well, if you ever need to compute f of n again, here it is. And then we return that value. So this be a general procedure. It can apply to any recursive algorithm with no side effect I guess, technically. And it turn out, this make the algorithm efficient. Now there's a lot of way to see why it's efficient. In general, maybe it's helpful to think about the recursion tree. So if you want to compute fn in the old algorithm, we compute fn minus completely separately. To compute fn minus 1 we compute fn minus 2 and fn minus 3. To compute fn minus 2 we compute fn minus 3 and fn minus 4. And so on. And you can see why that's exponential in n. Because we're only decrementing n by one or two each time. But then you observe, hey, these fn minus 3's be the same. I should really only have to compute them once. And that's what we're do here. The first time you call fn minus 3, you do work. But once it's do and you go over to this other recursive call, this will just get cut off. There's no tree here. Here we might have some recursive calling. Here we won't, because it's already in the memo table. In fact, this already happens with fn minus 2. This whole tree disappears because fn minus 2 have already be done. OK. So it's clear why it improves things. So in fact you can argue that this call will be free because you already do the work in here. But I want to give you a very particular way of think about why this be efficient, which be following. So you could write down a recurrence for the run time here. But in some sense recurrence aren't quite the right way of think about this because recursion be kind of a rare thing. If you're call Fibonacci of some value, k, you're only go to make recursive call the first time you call Fibonacci of k. Because henceforth, you've put it in the memo table you will not recurse. So you can think of there be two version of call Fibonacci of k. There's the first time, which be the non-memoized version that do recursion-- do some work. And then every time henceforth you're do memoized call of Fibonacci of k, and those cost constant time. So the memoized call cost constant time. So we can think of them a basically free. That's when you call Fibonacci of n minus 2, because that's a memoized call, you really don't pay anything for it. I mean, you're already pay constant time to do addition and whatever. So you don't have to worry about the time. There's no recursion here. And then what we care about be that the number of non-memorized calls, which be the first time you call Fibonacci of k, be n. No theta be even necessary. We be go to call Fibonacci of 1. At some point we're go to call Fibonacci of 2 at some point, and the original call be Fibonacci of n. All of those thing will be call at some point. That's pretty easy to see. But in particular, certainly at most this, we never call Fibonacci of n plus 1 to compute Fibonacci of n. So it's at most n calls. Indeed it will be exactly n call that be not memoized. Those one we have to pay for. How much do we have to pay? Well, if you don't count the recursion-- which be what this recurrence does-- if you ignore recursion then the total amount of work do here be constant. So I will say the non-recursive work per call be constant. And therefore I claim that the run time be constant-- I'm sorry, be linear. Constant would be pretty amazing. This be actually not the best algorithm-- a an aside. The best algorithm for compute the nth Fibonacci number us log n arithmetic operations. So you can do better, but if you want to see that you should take 6046. OK. We're just go to get to linear today, which be a lot good than exponential. So why linear? Because there's n non-memoize calls, and each of them cost constant. So it's the product of those two numbers. This be an important idea. And it's so important I'm go to write it down again in a slightly more general framework. In general, in dynamic programming-- I didn't say why it's call memoization. The idea be you have this memo pad where you write down all your scratch work. That's this memo dictionary. And to memoize be to write down on your memo pad. I didn't make it up. Another crazy term. It mean remember. And then you remember all the solution that you've done. And then you reuse those solutions. Now these solution be not really a solution to the problem that I care about. The problem I care about be compute the nth Fibonacci number. To get there I have to compute other Fibonacci numbers. Why? Because I have a recursive formulation. This be not always the way to solve a problem. But usually when you're solve something you can split it into parts, into subproblems, we call them. They're not always of the same flavor a your original goal problem, but there's some kind of related parts. And this be the big challenge in design a dynamic program, be to figure out what be the subproblems. Let's say, the first thing I want to know about a dynamic program, be what be the subproblems. Somehow they be design to help solve your actual problem. And the idea of memoization is, once you solve a subproblem, write down the answer. If you ever need to solve that same problem again you reuse the answer. So that be the core idea. And so in this sense dynamic program be essentially recursion plus memoization. And so in this case these be the subproblems. Fibonacci of 1 through Fibonacci of n. The one we care about be Fibonacci of n. But to get there we solve these other subproblems. In all cases, if this be the situation-- so for any dynamic program, the run time be go to be equal to the number of different subproblems you might have to solve, or that you do solve, time the amount of time you spend per subproblem. OK. In this situation we have n subproblems. And for each of them we spent constant time. And when I measure the time per subproblem which, in the Fibonacci case I claim be constant, I ignore recursive calls. That's the key. We don't have to solve recurrence with dynamic programming. Yay. No recurrence necessary. OK. Don't count recursions. Obviously, don't count memoized recursions. The reason is, I only need to count them once. After the first time I do it, it's free. So I count how many different subproblems do I need to do? These be they go to be the expensive recursion where I do work, I do some amount of work, but I don't count the recursion because otherwise I'd be double counting. I only want to count each subproblem once, and then this will solve it. So a simple idea. In general, dynamic program be a super simple idea. It's nothing fancy. It's basically just memoization. There be one extra trick we're go to pull out, but that's the idea. All right. Let me tell you another perspective. This be the one maybe most commonly taught. Is to think of-- but I'm not a particular fan of it. I really like memoization. I think it's a simple idea. And a long a you remember this formula here, it's really easy to work with. But some people like to think of it this way. And so you can pick whichever way you find most intuitive. Instead of think of a recursive algorithm, which in some sense start at the top of what you want to solve and work it way down, you could do the reverse. You could start at the bottom and work your way up. And this be probably how you normally think about compute Fibonacci number or how you learn it before. I'm go to write it in a slightly funny way. The point I want to make be that the transformation I'm do from the naive recursive algorithm, to the memoized algorithm, to the bottom-up algorithm be completely automated. I'm not thinking, I'm just doing. OK. It's easy. This code be exactly the same a this code and a that code, except I replace n by k. Just because I need a couple of different n value here. Or I want to iterate over n values. And then there's this stuff around that code which be just formulaic. A little bit of thought go into this for loop, but that's it. OK. This do exactly the same thing a the memoized algorithm. Maybe it take a little bit of think to realize, if you unroll all the recursion that's happen here and just write it out sequentially, this be exactly what's happening. This code do exactly the same additions, exactly the same computation a this. The only difference be how you get there. Here we're use a loop, here we're use recursion. But the same thing happen in the same order. It's really no difference between the code. This code's probably go to be more efficient practice because you don't make function call so much. In fact I make a little mistake here. This be not a function call, it's just a lookup into a table. Here I'm use a hash table to be simple, but of course you could use an array. But they're both constant time with good hashing. All right. So be it clear what this be doing? I think so. I think I make a little typo. So we have to compute-- oh, another typo. We have to compute f1 up to fn, which in python be that. And we compute it exactly how we use to. Except now, instead of recursing, I know that when I'm compute the k Fibonacci number-- man. So many typos. AUDIENCE: [LAUGHTER] PROFESSOR: You guy be laughing. When I compute the kth Fibonacci number I know that I've already compute the previous two. Why? Because I'm do them in increase order. Nothing fancy. Then I can just do this and the solution will just be wait there. If they work, I'd get a key error. So I'd know that there's a bug. But in fact, I won't get a key error. I will have always compute these thing already. Then I store it in my table. Then I iterate. Eventually I've solve all the subproblems, f1 through fn. And the one I care about be the nth one. OK. So straightforward. I do this because I don't really want to have to go through this transformation for every single problem we do. I'm do it in Fibonacci because it's super easy to write the code out explicitly. But you can do it for all of the dynamic program that we cover in the next four lectures. OK. I'm go to give you now the general case. This be the special Fibonacci version. In general, the bottom-up do exactly the same computation a the memoized version. And what we're do be actually a topological sort of the subproblem dependency DAG. So in this case, the dependency DAG be very simple. In order to compute-- I'll do it backwards. In order to compute fn, I need to know fn minus 1 and fn minus 2. If I know those I can compute fn. Then there's fn minus 3, which be necessary to compute this one, and that one, and so on. So you see what this DAG look like. Now, I've drawn it conveniently so all the edge go left to right. So this be a topological order from left to right. And so I just need to do f1, f2, up to fn in order. Usually it's totally obvious what order to solve the subproblems in. But in general, what you should have in mind be that we be do a topological sort. Here we just do it in our head because it's so easy. And usually it's so easy. It's just a for loop. Nothing fancy. All right. I'm miss an arrow. All right. Let's do something a little more interesting, shall we? All right. One thing you can do from this bottom-up perspective be you can save space. Storage space in the algorithm. We don't usually worry about space in this class, but it matter in reality. So here we're building a table size, n, but in fact we really only need to remember the last two values. So you could just store the last two values, and each time you make a new one delete the oldest. so by think a little bit here you realize you only need constant space. Still linear time, but constant space. And that's often the case. From the bottom-up perspective you see what you really need to store, what you need to keep track of. All right. I guess another nice thing about this perspective is, the run time be totally obvious. This be clearly constant time. So this be clearly linear time. Whereas, in this memoized algorithm you have to think about, when's it go to be memoized, when be it not? I still like this perspective because, with this rule, just multiply a number of subproblems by time per subproblem, you get the answer. But it's a little less obvious than code like this. So choose however you like to think about it. All right. We move onto shortest paths. So I'm again, a usual, think about single-source shortest paths. So we want to compute the shortest pathway from s to v for all v. OK. I'd like to write this initially a a naive recursive algorithm, which I can then memoize, which I can then bottom-upify. I just make that up. So how could I write this a a naive recursive algorithm? It's not so obvious. But first I'm go to tell you how, just a an oracle tell you, here's what you should do. But then we're go to think about-- go back, step back. Actually, it's up to you. I could tell you the answer and then we could figure out how we get there, or we could just figure out the answer. Preferences? Figure it out. All right. Good. No divine inspiration allowed. So let me give you a tool. The tool be guessing. This may sound silly, but it's a very powerful tool. The general idea is, suppose you don't know something but you'd like to know it. So what's the answer to this question? I don't know. Man, I really want a cushion. How be I go to answer the question? Guess. OK? AUDIENCE: [LAUGHTER] PROFESSOR: It's a try and test method for solve any problem. I'm kind of belaboring the point here. The algorithmic concept is, don't just try any guess. Try them all. OK? AUDIENCE: [LAUGHTER] PROFESSOR: Also pretty simple. I say dynamic program be simple. OK. Try all guesses. This be central to the dynamic programming. I know it sound obvious, but if I want to fix my equation here, dynamic program be roughly recursion plus memoization. This should really be, plus guessing. Memoization, which be obvious, guess which be obvious, be the central concept to dynamic programming. I'm try to make it sound easy because usually people have trouble with dynamic programming. It be easy. Try all the guesses. That's something a computer can do great. This be the brute force part. OK. But we're go to do it carefully. Not that carefully. I mean, we're just try all the guesses. Take the best one. That's kind of important that we can choose one to be call best. That's why dynamic program be good for optimization problems. You want to maximize something, minimize something, you try them all and then you can forget about all of them and just reduce it down to one thing which be the best one, or a best one. OK. So now I want you to try to apply this principle to shortest paths. Now I'm go to draw a picture which may help. We have the source, s, we have some vertex, v. We'd like to find the shortest-- a shortest path from s to v. Suppose I want to know what this shortest path is. Suppose this be it. You have an idea already? Yeah. AUDIENCE: What you could do be you could look at everywhere you can go from s. [INAUDIBLE] shortest path of each of those notes. PROFESSOR: Good. So I can look at all the place I could go from s, and then look at the shortest path from there to v. So we could call this s prime. So here's the idea. There's some hypothetical shortest path. I don't know where it go first, so I will guess where it go first. I know the first edge must be one of the outgo edge from s. I don't know which one. Try them all. Very simple idea. Then from each of those, if somehow I can compute the shortest path from there to v, just do that and take the best choice for what that first edge was. So this would be the guess first edge approach. It's a very good idea. Not quite the one I want because unfortunately that change s. And so this would work, it would just be slightly less efficient if I'm solve single-source shortest paths. So I'm go to tweak that idea slightly by guess the last edge instead of the first edge. They're really equivalent. If I be do this I'd essentially be solve a single-target shortest paths, which we talk about before. So I'm go to draw the same picture. I want to get to v. I'm go to guess the last edge, call it uv. I know it's one of the incoming edge to v-- unless s equal v, then there's a special case. As long a this path have length of at least 1, there's some last edge. What be it? I don't know. Guess. Guess all the possible incoming edge to v, and then recursively compute the shortest path from s to u. And then add on the edge v. OK. So what be this shortest path? It's delta of s comma u, which look the same. It's another subproblem that I want to solve. There's v subproblems here I care about. . So that's good. I take that. I add on the weight of the edge uv. And that should hopefully give me delta of s comma v. Well, if I be lucky and I guess the right choice of u. In reality, I'm not lucky. So I have to minimize over all edge uv. So this be the-- we're minimize over the choice of u. V be already give here. So I take the minimum over all edge of the shortest path from s to u, plus the weight of the edge uv. That should give me the shortest path because this give me the shortest path from s to u. Then I add on the edge I need to get there. And wherever the shortest path is, it us some last edge, uv. There's get to be some choice of u that be the right one. That's the good guess that we're hop for. We don't know what the good guess be so we just try them all. But whatever it is, this will be the weight of that path. It's go to take the best path from s to u because sub path be shortest path be shortest paths. Optimal substructure. So this part will be delta of su. This part be obviously w of uv. So this will give the right answer. Hopefully. OK. It's certainly go to-- I mean, this be the analog of the naive recursive algorithm for Fibonacci. So it's not go to be efficient if I-- I mean, this be an algorithm, right? You could say-- this be a recursive call. We're go to treat this a recursive call instead of just a definition. Then this be a recursive algorithm. How good or bad be this recursive algorithm? AUDIENCE: Terrible. PROFESSOR: Terrible. Very good. Very bad, I should say. It's definitely go to be exponential without memoization. But we know. We know how to make algorithm better. We memoize. OK. So I think you know how to write this a a memoized algorithm. To define the function delta of sv, you first check, be s comma v in the memo table? If so return that value. Otherwise, do this computation where this be a recursive call and then store it in the memo table. OK. I don't think I need to write that down. It's just like the memoized code over there. Just there's now two argument instead of one. In fact, s isn't changing. So I only need to store with v instead of s comma v. Is that a good algorithm? I claim memoization make everything faster. Is that a fast algorithm? Not so obvious, I guess. Yes? How many people think, yes, that's a good algorithm? AUDIENCE: Better. PROFESSOR: Better. Definitely better. Can't be worse. How many people think it's a bad algorithm still? OK. So three for yes, zero for no. How many people aren't sure? Including the yes votes? Good. All right. It's not so tricky. Let me draw you a graph. Something like that. So we want to commit delta of s comma v. Let me give these guy names, a and b. So we compute delta of s comma v. To compute that we need to know delta of s comma a and delta of s comma v. All right? Those be the two ways-- sorry, actually we just need one. Only one incoming edge to v. So it delta of s comma a. Sorry-- I should have put a base case here too. Delta of s comma s equal 0. OK. Delta of s comma a plus the edge. OK. There be some shortest path to a. To compute the shortest path to a we look at all the incoming edge to a. There's only one. So delta of s comma b. Now I want to compute the shortest path from b. Well, there's two way to get to b. One of them be delta of s comma b-- sorry, s comma s. Came from s. The other way be delta of s comma v. Do you see a problem? Yeah. Delta of s comma v be what we be try to figure out. Now you might say, oh, it's OK because we're go to memoize our answer to delta s comma v and then we can reuse it here. Except, we haven't finish compute delta of s comma v. We can only put it in the memo table once we're done. So when this call happens the memo table have not be set. And we're go to do the same thing over and over and over again. This be an infinite algorithm. Oops. Not so hot. So it's go to be infinite time on graph with cycles. OK. For DAGs, for acyclic graphs, it actually run in v plus e time. This be the good case. In this situation we can use this formula. The time be equal to the number of subproblems time the time per subproblem. So I guess we have to think about that a little bit. Where's my code? Here's my code. Number of subproblems be v. There's v different subproblems that I'm use here. I'm always reuse subproblems of the form delta s comma something. The something could be any of the v vertices. How much time do I spend per subproblem? That's a little tricky. It's the number of incoming edge to v. So time for a sub problem delta of sv be the indegree of v. The number of incoming edge to v. So this depends on v. So I can't just take a straightforward product here. What this be really say is, you should sum up over all sub problem of the time per sub problem. So total time be the sum over all v and v, the indegree of v. And we know this be number of edges. It's really-- so indegree plus 1, indegree plus 1. So this be v plus v. OK. Handshaking again. OK. Now we already knew an algorithm for shortest path and DAGs. And it ran a v plus e time. So it's another way to do the same thing. If you think about it long enough, this algorithm memoized, be essentially do a depth first search to do a topological sort to run one round of Bellman-Ford. So we have topological sort plus one round of Bellman-Ford. This be kind of it all roll into one. This should look kind of like the Bellman Ford relaxation step, or shortest path relaxation step. It is. This min be really do the same thing. So it's really the same algorithm. But we come at it from a different perspective. OK. But I claim I can use this same approach to solve shortest path in general graphs, even when they have cycles. How be I go to do that? DAGs seem fine-- oh, what be the lesson learn here? Lesson learn be that subproblem dependency should be acyclic. Otherwise, we get an infinite algorithm. For memoization to work this be what you need. It's all you need. OK. We've almost see this already. Because I say that, to do a bottom up algorithm you do a topological sort of this subproblem dependency DAG. I already say it should be acyclic. OK. We just forgot. I didn't tell you yet. So for that to work it good be acyclic. For DP to work, for memoization to work, it good be acyclic. If you're acyclic then this be the run time. So that's all general. OK. So somehow I need to take a cyclic graph and make it acyclic. We've actually do this already in recitation. So if I have a graph-- let's take a very simple cyclic graph. OK. One thing I could do be explode it into multiple layers. We do this on quiz two in various forms. It's like the only cool thing you can do with shortest paths, I feel like. If you want to make a shortest path problem harder, require that you reduce your graph to k copy of the graph. I'm go to do it in a particular way here-- which I think you've see in recitation-- which be to think of this axis a time, or however you want, and make all of the edge go from each layer to the next layer. This should be a familiar technique. So the idea is, every time I follow an edge I go down to the next layer. This make any graph acyclic. Done. What in the world do this mean? What be it doing? What do it mean? Double rainbow. All right. AUDIENCE: [LAUGHTER] PROFESSOR: So-- I don't know how I've go so long in the semester without refer to double rainbow. It use to be my favorite. All right. So here's what it means. Delta sub k of sv. I'm go to define this first-- this be a new kind of subproblem-- which is, what be the shortest-- what be the weight of the shortest s to v path that uses, at most, k edges. So I want it to be shortest in term of total weight, but I also want it to use few edge total. So this be go to be 0. In some sense, if you look at-- so here's s and I'm always go to make s this. And then this be go to be v in the zero situation. This be go to be v in the one situation, v-- so if I look at this v, I look at the shortest path from s to v, that be delta sub 0 of sv. So maybe I'll call this v sub 0, v sub 1, v sub 2. OK. Shortest path from here to here is, there's no way to get there on 0 edges. Shortest path from here to here, that be the best way to get there with, at most, one edge. Shortest path from here to here-- well, if I add some vertical edge too, I guess, cheat a little bit. Then this be the best way to get from s to v use at most two edges. And then you get a recurrence which be the min over all last edges. So I'm just copying that recurrence, but realize that the s to u part us one few edge. And then I use the edge uv. OK. That's our new recurrence. By add this k parameter I've make this recurrence on subproblems acyclic. Unfortunately, I've increase the number of subproblems. The number of subproblems now be v squared. Technically, v time v minus 1. Because I really-- actually, v squared. Sorry. I start at 0. And what I care about, my goal, be delta sub v minus 1 of sv. Because by Bellman-Ford analysis I know that I only care about simple paths, path of length at most v minus 1. I'm assume here no negative weight cycles. I should've say that earlier. If you assume that, then this be what I care about. So k range from 0 to v minus 1. So there be v choice for k. There be v choice for v. So the number of subproblems be v squared. How much time do I spend per subproblem? Well, the same a before. The indegree-- where do I write it? Up here-- the indegree of that problem. So what I'm really do be sum over all v of the indegree. And then I multiply it by v. So the run time, total run time be ve. Sound familiar? This be Bellman-Ford's algorithm again. And this be actually where Bellman-Ford algorithm come from be this view on dynamic programming. So we're see yet another way to do Bellman-Ford. It may seem familiar. But in the next three lecture we're go to see a whole bunch of problem that can succumb to the same approach. And that's super cool. 