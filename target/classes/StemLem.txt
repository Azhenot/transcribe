the follow content be provid under a creativ common license. your support will help mit opencoursewar continu to offer high qualiti educ resourc for free. To make a donat or view addit materi from hundr of mit courses, visit mit opencoursewar at ocw.mit.edu. professor: we'r go to start a brand new, excit topic, dynam programming. audience: yes! professor: yeah! So exciting. actually, I be realli excit becaus dynam program be my favorit thing in the world, in algorithms. and it' go to be the next four lectures, it' so exciting. It have lot of differ facets. it' a veri general, power design technique. We don't talk a lot about algorithm design in thi class, but dynam program be one that' so important. and also take a littl while to settl in. We like to inject it into you now, in 006. So in general, our motiv be design new algorithm and dynam programming, also call dp, be a great way-- or a veri general, power way to do this. it' especi good, and intend for, optim problems, thing like shortest paths. you want to find the best way to do something. shortest path be you want to find the shortest path, the minimum-length path. you want to minimize, maxim something, that' an optim problem, and typic good algorithm to solv them involv dynam programming. it' a bit of a broad statement. you can also think of dynam program a a kind of exhaust search. which be usual a bad thing to do becaus it lead to exponenti time. but if you do it in a clever way, via dynam programming, you typic get polynomi time. So one perspect be that dynam program be approxim care brute force. it' kind of a funni combination. A bit of an oxymoron. but we take the idea of brute force, which is, tri all possibl and you do it care and you get it to polynomi time. there be a lot of problem where essenti the onli know polynomi time algorithm be via dynam programming. It doesn't alway work, there' some problem where we don't think there be polynomi time algorithms, but when it' possibl DP be a nice, sort of, gener approach to it. and we'r go to be talk a lot about dynam programming. there' a lot of differ way to think about it. we'll look at a few today. we'r go to warm up today with some fairli easi problem that we alreadi know how to solve, name comput fibonacci numbers. it' pretti easy. and comput shortest paths. and then in the next three lectur we'r go to get to more interest exampl where it' pretti surpris that you can even solv the problem in polynomi time. probabl the first burn question on your mind, though, be whi be it call dynam programming? what do that even mean? and I use to have thi spiel about, well, you know, program refer to the-- I think it' the british notion of the word, where it' about optimization. optim in american english be someth like program in british english, where you want to set up the program-- the schedul for your train or something, where program come from originally. but I look up the actual histori of, whi be it call dynam programming. dynam program be invent by a guy name richard bellman. you may have heard of bellman in the bellman-ford algorithm. So thi be actual the precursor to bellman-ford. and we'r go to see bellman-ford come up natur in thi setting. So here' a quot about him. It says, bellman explain that he invent the name dynam program to hide the fact that he be do mathemat research. He be work at thi place call rand, and under a secretari of defens who have a patholog fear and hatr for the term research. So he settl on the term dynam program becaus it would be difficult to give a pejor mean to it. and becaus it be someth not even a congressman could object to. basically, it sound cool. So that' the origin of the name dynam programming. So whi be the call that? who knows. I mean, now you know. but it' not-- it' a weird term. just take it for what it is. It may make some kind of sense, but-- all right. So we be go to start with thi exampl of how to comput fibonacci numbers. and mayb befor we actual start i'm go to give you a sneak peak of what you can think of dynam program as. and thi equation, so to speak, be go to chang throughout today' lecture. In the end we'll settl on a sort of more accur perspective. the basic idea of dynam program be to take a problem, split it into subproblems, solv those subproblems, and reus the solut to your subproblems. it' like a lesson in recycling. So we'll see that in fibonacci numbers. So you rememb fibonacci numbers, right? the number of rabbit you have on day n, if they reproduce. we'v mention them before, we'r talk about avl trees, I think. So thi be the usual-- you can think of it a a recurs definit or recurr on fibonacci numbers. it' the definit of what the nth fibonacci number is. So let' suppos our goal-- an algorithm problem is, comput the nth fibonacci number. and i'm go to assum here that that fit in a word. and so basic arithmetic, addition, whatever' constant time per operation. So how do we do it? you all know how to do it. anyways-- but i'm go to give you the dynam program perspect on things. So thi will seem kind of obvious, but it is-- we'r go to appli exactli the same principl that we will appli over and over in dynam programming. but here it' in a veri familiar setting. So we'r go to start with the naiv recurs algorithm. and that is, if you want to comput the nth fibonacci number, you check whether you'r in the base case. i'm go to write it thi way. So f be just our return value. you'll see whi I write it thi way in a moment. then you return f. In the base case it' 1, otherwis you recurs call fibonacci of n minu 1. you recurs call fibonacci of n minu 2. add them together, return that. thi be a correct algorithm. Is it a good algorithm? no. it' veri bad. exponenti time. how do we know it' exponenti time, other than from experience? well, we can write the run time a recurrence. T of n repres the time to comput the nth fibonacci number. how can I write the recurrence? you'r gonna throwback to the earli lectures, divid and conquer. I hear whispers. yeah? audience: [inaudible]. professor: yeah. T of n minu 1 plu t of n minu 2 plu constant. I don't know how mani you have by now. So to creat the nth fibonacci number we have to comput the n minu first fibonacci number, and the n minu second fibonacci number. that' these two recursions. and then we take constant time otherwise. We do constant number of additions, comparisons. return all these operations-- take constant time. So that' a recurrence. how do we solv thi recurrence? well one way be to see thi be the fibonacci recurrence. So it' the same thing. there' thi plu whatever. but in particular, thi be at least the nth fibonacci number. and if you know fibonacci stuff, that' about the golden ratio to the nth power. which be bad. We have a similar recurr in avl trees. and so anoth way to solv it-- it' just good review-- say, oh well, that' at least 2 time t of n minu 2. becaus it' go to be monotone. the big n is, the more work you have to do. becaus to do the nth thing you have to do the n minu first thing. So we could just reduc t of n minu 1 to t of n minu 2. that will give u a low bound. and now these two terms-- now thi be sort of an easi thing. you see that you'r multipli by 2 each time. you'r subtract how mani time can I subtract 2 from n? n/2 times, befor I get down to a constant. and so thi be equal to 2 to the n over 2-- I mean, time some constant, which be what you get in the base case. So I guess I should say theta. thi thing be theta that. ok. So it' at least that big. and the right constant be phi. and the base of the exponent. ok. So that' a bad algorithm. We all know it' a bad algorithm. but i'm go to give you a gener approach for make bad algorithm like thi good. and that gener approach be call memoization. we'll go over here. and thi be a techniqu of dynam programming. So i'm go to call thi the memoiz dynam program algorithm. So do I settl on use memo in the notes? yeah. the idea be simple. whenev we comput a fibonacci number we put it in a dictionary. and then when we need to comput the nth fibonacci number we check, be it alreadi in the dictionary? did we alreadi solv thi problem? If so, return that answer. otherwise, comput it. you'll see the transform be veri simple. ok. these two line be ident to these two lines. So you can see how the transform work in general. you could do thi with ani recurs algorithm. the memoiz transform on that algorithm-- which is, we initi make an empti dictionari call memo. and befor we actual do the comput we say, well, check whether thi version of the fibonacci problem, comput f of n, be alreadi in our dictionary. So if that key be alreadi in the dictionary, we return the correspond valu in the dictionary. and then onc we'v comput the nth fibonacci number, if we bother to do this, if thi didn't apply, then we store it in the memo table. So we say well, if you ever need to comput f of n again, here it is. and then we return that value. So thi be a gener procedure. It can appli to ani recurs algorithm with no side effect I guess, technically. and it turn out, thi make the algorithm efficient. now there' a lot of way to see whi it' efficient. In general, mayb it' help to think about the recurs tree. So if you want to comput fn in the old algorithm, we comput fn minu complet separately. To comput fn minu 1 we comput fn minu 2 and fn minu 3. To comput fn minu 2 we comput fn minu 3 and fn minu 4. and so on. and you can see whi that' exponenti in n. becaus we'r onli decrement n by one or two each time. but then you observe, hey, these fn minu 3' be the same. I should realli onli have to comput them once. and that' what we'r do here. the first time you call fn minu 3, you do work. but onc it' do and you go over to thi other recurs call, thi will just get cut off. there' no tree here. here we might have some recurs calling. here we won't, becaus it' alreadi in the memo table. In fact, thi alreadi happen with fn minu 2. thi whole tree disappear becaus fn minu 2 have alreadi be done. ok. So it' clear whi it improv things. So in fact you can argu that thi call will be free becaus you alreadi do the work in here. but I want to give you a veri particular way of think about whi thi be efficient, which be following. So you could write down a recurr for the run time here. but in some sens recurr aren't quit the right way of think about thi becaus recurs be kind of a rare thing. If you'r call fibonacci of some value, k, you'r onli go to make recurs call the first time you call fibonacci of k. becaus henceforth, you'v put it in the memo tabl you will not recurse. So you can think of there be two version of call fibonacci of k. there' the first time, which be the non-memo version that do recursion-- do some work. and then everi time henceforth you'r do memoiz call of fibonacci of k, and those cost constant time. So the memoiz call cost constant time. So we can think of them a basic free. that' when you call fibonacci of n minu 2, becaus that' a memoiz call, you realli don't pay anyth for it. I mean, you'r alreadi pay constant time to do addit and whatever. So you don't have to worri about the time. there' no recurs here. and then what we care about be that the number of non-memor calls, which be the first time you call fibonacci of k, be n. No theta be even necessary. We be go to call fibonacci of 1. At some point we'r go to call fibonacci of 2 at some point, and the origin call be fibonacci of n. all of those thing will be call at some point. that' pretti easi to see. but in particular, certainli at most this, we never call fibonacci of n plu 1 to comput fibonacci of n. So it' at most n calls. inde it will be exactli n call that be not memoized. those one we have to pay for. how much do we have to pay? well, if you don't count the recursion-- which be what thi recurr does-- if you ignor recurs then the total amount of work do here be constant. So I will say the non-recurs work per call be constant. and therefor I claim that the run time be constant-- i'm sorry, be linear. constant would be pretti amazing. thi be actual not the best algorithm-- a an aside. the best algorithm for comput the nth fibonacci number us log n arithmet operations. So you can do better, but if you want to see that you should take 6046. ok. we'r just go to get to linear today, which be a lot good than exponential. So whi linear? becaus there' n non-memo calls, and each of them cost constant. So it' the product of those two numbers. thi be an import idea. and it' so import i'm go to write it down again in a slightli more gener framework. In general, in dynam programming-- I didn't say whi it' call memoization. the idea be you have thi memo pad where you write down all your scratch work. that' thi memo dictionary. and to memoiz be to write down on your memo pad. I didn't make it up. anoth crazi term. It mean remember. and then you rememb all the solut that you'v done. and then you reus those solutions. now these solut be not realli a solut to the problem that I care about. the problem I care about be comput the nth fibonacci number. To get there I have to comput other fibonacci numbers. why? becaus I have a recurs formulation. thi be not alway the way to solv a problem. but usual when you'r solv someth you can split it into parts, into subproblems, we call them. they'r not alway of the same flavor a your origin goal problem, but there' some kind of relat parts. and thi be the big challeng in design a dynam program, be to figur out what be the subproblems. let' say, the first thing I want to know about a dynam program, be what be the subproblems. somehow they be design to help solv your actual problem. and the idea of memoiz is, onc you solv a subproblem, write down the answer. If you ever need to solv that same problem again you reus the answer. So that be the core idea. and so in thi sens dynam program be essenti recurs plu memoization. and so in thi case these be the subproblems. fibonacci of 1 through fibonacci of n. the one we care about be fibonacci of n. but to get there we solv these other subproblems. In all cases, if thi be the situation-- so for ani dynam program, the run time be go to be equal to the number of differ subproblem you might have to solve, or that you do solve, time the amount of time you spend per subproblem. ok. In thi situat we have n subproblems. and for each of them we spent constant time. and when I measur the time per subproblem which, in the fibonacci case I claim be constant, I ignor recurs calls. that' the key. We don't have to solv recurr with dynam programming. yay. No recurr necessary. ok. don't count recursions. obviously, don't count memoiz recursions. the reason is, I onli need to count them once. after the first time I do it, it' free. So I count how mani differ subproblem do I need to do? these be they go to be the expens recurs where I do work, I do some amount of work, but I don't count the recurs becaus otherwis i'd be doubl counting. I onli want to count each subproblem once, and then thi will solv it. So a simpl idea. In general, dynam program be a super simpl idea. it' noth fancy. it' basic just memoization. there be one extra trick we'r go to pull out, but that' the idea. all right. let me tell you anoth perspective. thi be the one mayb most commonli taught. Is to think of-- but i'm not a particular fan of it. I realli like memoization. I think it' a simpl idea. and a long a you rememb thi formula here, it' realli easi to work with. but some peopl like to think of it thi way. and so you can pick whichev way you find most intuitive. instead of think of a recurs algorithm, which in some sens start at the top of what you want to solv and work it way down, you could do the reverse. you could start at the bottom and work your way up. and thi be probabl how you normal think about comput fibonacci number or how you learn it before. i'm go to write it in a slightli funni way. the point I want to make be that the transform i'm do from the naiv recurs algorithm, to the memoiz algorithm, to the bottom-up algorithm be complet automated. i'm not thinking, i'm just doing. ok. it' easy. thi code be exactli the same a thi code and a that code, except I replac n by k. just becaus I need a coupl of differ n valu here. Or I want to iter over n values. and then there' thi stuff around that code which be just formulaic. A littl bit of thought go into thi for loop, but that' it. ok. thi do exactli the same thing a the memoiz algorithm. mayb it take a littl bit of think to realize, if you unrol all the recurs that' happen here and just write it out sequentially, thi be exactli what' happening. thi code do exactli the same additions, exactli the same comput a this. the onli differ be how you get there. here we'r use a loop, here we'r use recursion. but the same thing happen in the same order. it' realli no differ between the code. thi code' probabl go to be more effici practic becaus you don't make function call so much. In fact I make a littl mistak here. thi be not a function call, it' just a lookup into a table. here i'm use a hash tabl to be simple, but of cours you could use an array. but they'r both constant time with good hashing. all right. So be it clear what thi be doing? I think so. I think I make a littl typo. So we have to compute-- oh, anoth typo. We have to comput f1 up to fn, which in python be that. and we comput it exactli how we use to. except now, instead of recursing, I know that when i'm comput the k fibonacci number-- man. So mani typos. audience: [laughter] professor: you guy be laughing. when I comput the kth fibonacci number I know that i'v alreadi comput the previou two. why? becaus i'm do them in increas order. noth fancy. then I can just do thi and the solut will just be wait there. If they work, i'd get a key error. So i'd know that there' a bug. but in fact, I won't get a key error. I will have alway comput these thing already. then I store it in my table. then I iterate. eventu i'v solv all the subproblems, f1 through fn. and the one I care about be the nth one. ok. So straightforward. I do thi becaus I don't realli want to have to go through thi transform for everi singl problem we do. i'm do it in fibonacci becaus it' super easi to write the code out explicitly. but you can do it for all of the dynam program that we cover in the next four lectures. ok. i'm go to give you now the gener case. thi be the special fibonacci version. In general, the bottom-up do exactli the same comput a the memoiz version. and what we'r do be actual a topolog sort of the subproblem depend dag. So in thi case, the depend dag be veri simple. In order to compute-- i'll do it backwards. In order to comput fn, I need to know fn minu 1 and fn minu 2. If I know those I can comput fn. then there' fn minu 3, which be necessari to comput thi one, and that one, and so on. So you see what thi dag look like. now, i'v drawn it conveni so all the edg go left to right. So thi be a topolog order from left to right. and so I just need to do f1, f2, up to fn in order. usual it' total obviou what order to solv the subproblem in. but in general, what you should have in mind be that we be do a topolog sort. here we just do it in our head becaus it' so easy. and usual it' so easy. it' just a for loop. noth fancy. all right. i'm miss an arrow. all right. let' do someth a littl more interesting, shall we? all right. one thing you can do from thi bottom-up perspect be you can save space. storag space in the algorithm. We don't usual worri about space in thi class, but it matter in reality. So here we'r build a tabl size, n, but in fact we realli onli need to rememb the last two values. So you could just store the last two values, and each time you make a new one delet the oldest. so by think a littl bit here you realiz you onli need constant space. still linear time, but constant space. and that' often the case. from the bottom-up perspect you see what you realli need to store, what you need to keep track of. all right. I guess anoth nice thing about thi perspect is, the run time be total obvious. thi be clearli constant time. So thi be clearli linear time. whereas, in thi memoiz algorithm you have to think about, when' it go to be memoized, when be it not? I still like thi perspect because, with thi rule, just multipli a number of subproblem by time per subproblem, you get the answer. but it' a littl less obviou than code like this. So choos howev you like to think about it. all right. We move onto shortest paths. So i'm again, a usual, think about single-sourc shortest paths. So we want to comput the shortest pathway from s to v for all v. ok. i'd like to write thi initi a a naiv recurs algorithm, which I can then memoize, which I can then bottom-upify. I just make that up. So how could I write thi a a naiv recurs algorithm? it' not so obvious. but first i'm go to tell you how, just a an oracl tell you, here' what you should do. but then we'r go to think about-- go back, step back. actually, it' up to you. I could tell you the answer and then we could figur out how we get there, or we could just figur out the answer. preferences? figur it out. all right. good. No divin inspir allowed. So let me give you a tool. the tool be guessing. thi may sound silly, but it' a veri power tool. the gener idea is, suppos you don't know someth but you'd like to know it. So what' the answer to thi question? I don't know. man, I realli want a cushion. how be I go to answer the question? guess. ok? audience: [laughter] professor: it' a tri and test method for solv ani problem. i'm kind of belabor the point here. the algorithm concept is, don't just tri ani guess. tri them all. ok? audience: [laughter] professor: also pretti simple. I say dynam program be simple. ok. tri all guesses. thi be central to the dynam programming. I know it sound obvious, but if I want to fix my equat here, dynam program be roughli recurs plu memoization. thi should realli be, plu guessing. memoization, which be obvious, guess which be obvious, be the central concept to dynam programming. i'm tri to make it sound easi becaus usual peopl have troubl with dynam programming. It be easy. tri all the guesses. that' someth a comput can do great. thi be the brute forc part. ok. but we'r go to do it carefully. not that carefully. I mean, we'r just tri all the guesses. take the best one. that' kind of import that we can choos one to be call best. that' whi dynam program be good for optim problems. you want to maxim something, minim something, you tri them all and then you can forget about all of them and just reduc it down to one thing which be the best one, or a best one. ok. So now I want you to tri to appli thi principl to shortest paths. now i'm go to draw a pictur which may help. We have the source, s, we have some vertex, v. we'd like to find the shortest-- a shortest path from s to v. suppos I want to know what thi shortest path is. suppos thi be it. you have an idea already? yeah. audience: what you could do be you could look at everywher you can go from s. [inaudible] shortest path of each of those notes. professor: good. So I can look at all the place I could go from s, and then look at the shortest path from there to v. So we could call thi s prime. So here' the idea. there' some hypothet shortest path. I don't know where it go first, so I will guess where it go first. I know the first edg must be one of the outgo edg from s. I don't know which one. tri them all. veri simpl idea. then from each of those, if somehow I can comput the shortest path from there to v, just do that and take the best choic for what that first edg was. So thi would be the guess first edg approach. it' a veri good idea. not quit the one I want becaus unfortun that chang s. and so thi would work, it would just be slightli less effici if i'm solv single-sourc shortest paths. So i'm go to tweak that idea slightli by guess the last edg instead of the first edge. they'r realli equivalent. If I be do thi i'd essenti be solv a single-target shortest paths, which we talk about before. So i'm go to draw the same picture. I want to get to v. i'm go to guess the last edge, call it uv. I know it' one of the incom edg to v-- unless s equal v, then there' a special case. As long a thi path have length of at least 1, there' some last edge. what be it? I don't know. guess. guess all the possibl incom edg to v, and then recurs comput the shortest path from s to u. and then add on the edg v. ok. So what be thi shortest path? it' delta of s comma u, which look the same. it' anoth subproblem that I want to solve. there' v subproblem here I care about. . So that' good. I take that. I add on the weight of the edg uv. and that should hope give me delta of s comma v. well, if I be lucki and I guess the right choic of u. In reality, i'm not lucky. So I have to minim over all edg uv. So thi be the-- we'r minim over the choic of u. V be alreadi give here. So I take the minimum over all edg of the shortest path from s to u, plu the weight of the edg uv. that should give me the shortest path becaus thi give me the shortest path from s to u. then I add on the edg I need to get there. and wherev the shortest path is, it us some last edge, uv. there' get to be some choic of u that be the right one. that' the good guess that we'r hop for. We don't know what the good guess be so we just tri them all. but whatev it is, thi will be the weight of that path. it' go to take the best path from s to u becaus sub path be shortest path be shortest paths. optim substructure. So thi part will be delta of su. thi part be obvious w of uv. So thi will give the right answer. hopefully. ok. it' certainli go to-- I mean, thi be the analog of the naiv recurs algorithm for fibonacci. So it' not go to be effici if i-- I mean, thi be an algorithm, right? you could say-- thi be a recurs call. we'r go to treat thi a recurs call instead of just a definition. then thi be a recurs algorithm. how good or bad be thi recurs algorithm? audience: terrible. professor: terrible. veri good. veri bad, I should say. it' definit go to be exponenti without memoization. but we know. We know how to make algorithm better. We memoize. ok. So I think you know how to write thi a a memoiz algorithm. To defin the function delta of sv, you first check, be s comma v in the memo table? If so return that value. otherwise, do thi comput where thi be a recurs call and then store it in the memo table. ok. I don't think I need to write that down. it' just like the memoiz code over there. just there' now two argument instead of one. In fact, s isn't changing. So I onli need to store with v instead of s comma v. Is that a good algorithm? I claim memoiz make everyth faster. Is that a fast algorithm? not so obvious, I guess. yes? how mani peopl think, yes, that' a good algorithm? audience: better. professor: better. definit better. can't be worse. how mani peopl think it' a bad algorithm still? ok. So three for yes, zero for no. how mani peopl aren't sure? includ the ye votes? good. all right. it' not so tricky. let me draw you a graph. someth like that. So we want to commit delta of s comma v. let me give these guy names, a and b. So we comput delta of s comma v. To comput that we need to know delta of s comma a and delta of s comma v. all right? those be the two ways-- sorry, actual we just need one. onli one incom edg to v. So it delta of s comma a. sorry-- I should have put a base case here too. delta of s comma s equal 0. ok. delta of s comma a plu the edge. ok. there be some shortest path to a. To comput the shortest path to a we look at all the incom edg to a. there' onli one. So delta of s comma b. now I want to comput the shortest path from b. well, there' two way to get to b. one of them be delta of s comma b-- sorry, s comma s. came from s. the other way be delta of s comma v. Do you see a problem? yeah. delta of s comma v be what we be tri to figur out. now you might say, oh, it' OK becaus we'r go to memoiz our answer to delta s comma v and then we can reus it here. except, we haven't finish comput delta of s comma v. We can onli put it in the memo tabl onc we'r done. So when thi call happen the memo tabl have not be set. and we'r go to do the same thing over and over and over again. thi be an infinit algorithm. oops. not so hot. So it' go to be infinit time on graph with cycles. ok. for dags, for acycl graphs, it actual run in v plu e time. thi be the good case. In thi situat we can use thi formula. the time be equal to the number of subproblem time the time per subproblem. So I guess we have to think about that a littl bit. where' my code? here' my code. number of subproblem be v. there' v differ subproblem that i'm use here. i'm alway reus subproblem of the form delta s comma something. the someth could be ani of the v vertices. how much time do I spend per subproblem? that' a littl tricky. it' the number of incom edg to v. So time for a sub problem delta of sv be the indegre of v. the number of incom edg to v. So thi depend on v. So I can't just take a straightforward product here. what thi be realli say is, you should sum up over all sub problem of the time per sub problem. So total time be the sum over all v and v, the indegre of v. and we know thi be number of edges. it' really-- so indegre plu 1, indegre plu 1. So thi be v plu v. ok. handshak again. ok. now we alreadi knew an algorithm for shortest path and dags. and it ran a v plu e time. So it' anoth way to do the same thing. If you think about it long enough, thi algorithm memoized, be essenti do a depth first search to do a topolog sort to run one round of bellman-ford. So we have topolog sort plu one round of bellman-ford. thi be kind of it all roll into one. thi should look kind of like the bellman ford relax step, or shortest path relax step. It is. thi min be realli do the same thing. So it' realli the same algorithm. but we come at it from a differ perspective. ok. but I claim I can use thi same approach to solv shortest path in gener graphs, even when they have cycles. how be I go to do that? dag seem fine-- oh, what be the lesson learn here? lesson learn be that subproblem depend should be acyclic. otherwise, we get an infinit algorithm. for memoiz to work thi be what you need. it' all you need. ok. we'v almost see thi already. becaus I say that, to do a bottom up algorithm you do a topolog sort of thi subproblem depend dag. I alreadi say it should be acyclic. ok. We just forgot. I didn't tell you yet. So for that to work it good be acyclic. for DP to work, for memoiz to work, it good be acyclic. If you'r acycl then thi be the run time. So that' all general. ok. So somehow I need to take a cyclic graph and make it acyclic. we'v actual do thi alreadi in recitation. So if I have a graph-- let' take a veri simpl cyclic graph. ok. one thing I could do be explod it into multipl layers. We do thi on quiz two in variou forms. it' like the onli cool thing you can do with shortest paths, I feel like. If you want to make a shortest path problem harder, requir that you reduc your graph to k copi of the graph. i'm go to do it in a particular way here-- which I think you'v see in recitation-- which be to think of thi axi a time, or howev you want, and make all of the edg go from each layer to the next layer. thi should be a familiar technique. So the idea is, everi time I follow an edg I go down to the next layer. thi make ani graph acyclic. done. what in the world do thi mean? what be it doing? what do it mean? doubl rainbow. all right. audience: [laughter] professor: so-- I don't know how i'v go so long in the semest without refer to doubl rainbow. It use to be my favorite. all right. So here' what it means. delta sub k of sv. i'm go to defin thi first-- thi be a new kind of subproblem-- which is, what be the shortest-- what be the weight of the shortest s to v path that uses, at most, k edges. So I want it to be shortest in term of total weight, but I also want it to use few edg total. So thi be go to be 0. In some sense, if you look at-- so here' s and i'm alway go to make s this. and then thi be go to be v in the zero situation. thi be go to be v in the one situation, v-- so if I look at thi v, I look at the shortest path from s to v, that be delta sub 0 of sv. So mayb i'll call thi v sub 0, v sub 1, v sub 2. ok. shortest path from here to here is, there' no way to get there on 0 edges. shortest path from here to here, that be the best way to get there with, at most, one edge. shortest path from here to here-- well, if I add some vertic edg too, I guess, cheat a littl bit. then thi be the best way to get from s to v use at most two edges. and then you get a recurr which be the min over all last edges. So i'm just copi that recurrence, but realiz that the s to u part us one few edge. and then I use the edg uv. ok. that' our new recurrence. By add thi k paramet i'v make thi recurr on subproblem acyclic. unfortunately, i'v increas the number of subproblems. the number of subproblem now be v squared. technically, v time v minu 1. becaus I really-- actually, v squared. sorry. I start at 0. and what I care about, my goal, be delta sub v minu 1 of sv. becaus by bellman-ford analysi I know that I onli care about simpl paths, path of length at most v minu 1. i'm assum here no neg weight cycles. I should'v say that earlier. If you assum that, then thi be what I care about. So k rang from 0 to v minu 1. So there be v choic for k. there be v choic for v. So the number of subproblem be v squared. how much time do I spend per subproblem? well, the same a before. the indegree-- where do I write it? Up here-- the indegre of that problem. So what i'm realli do be sum over all v of the indegree. and then I multipli it by v. So the run time, total run time be ve. sound familiar? thi be bellman-ford' algorithm again. and thi be actual where bellman-ford algorithm come from be thi view on dynam programming. So we'r see yet anoth way to do bellman-ford. It may seem familiar. but in the next three lectur we'r go to see a whole bunch of problem that can succumb to the same approach. and that' super cool. 